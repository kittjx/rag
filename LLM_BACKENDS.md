# LLMåç«¯åˆ‡æ¢æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬ç³»ç»Ÿæ”¯æŒå¤šç§LLMåç«¯ï¼Œå¯ä»¥è½»æ¾åˆ‡æ¢ï¼š
- **DeepSeek** - é«˜æ€§ä»·æ¯”çš„ä¸­æ–‡å¤§æ¨¡å‹
- **Qwen (é€šä¹‰åƒé—®)** - é˜¿é‡Œäº‘çš„å¤§æ¨¡å‹æœåŠ¡
- **OpenAI** - GPTç³»åˆ—æ¨¡å‹
- **Ollama** - æœ¬åœ°è¿è¡Œçš„å¼€æºæ¨¡å‹

## ğŸ”§ é…ç½®æ–¹æ³•

### 1. ç¯å¢ƒå˜é‡é…ç½®

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```bash
# åç«¯é€‰æ‹©ç­–ç•¥
LLM_BACKEND=auto          # auto, deepseek, qwen, openai, ollama
USE_LOCAL_LLM=false       # trueä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollama

# DeepSeeké…ç½®
DEEPSEEK_API_KEY=sk-xxxxx
DEEPSEEK_MODEL=deepseek-chat

# Qwené…ç½®
QWEN_API_KEY=sk-xxxxx
QWEN_MODEL=qwen-turbo

# OpenAIé…ç½®
OPENAI_API_KEY=sk-xxxxx
OPENAI_MODEL=gpt-3.5-turbo

# Ollamaé…ç½®
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b
```

### 2. è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ (æ¨è)

è®¾ç½® `LLM_BACKEND=auto`ï¼Œç³»ç»Ÿä¼šæŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§è‡ªåŠ¨é€‰æ‹©ï¼š

1. å¦‚æœ `USE_LOCAL_LLM=true` â†’ ä½¿ç”¨ Ollama
2. å¦‚æœé…ç½®äº† `DEEPSEEK_API_KEY` â†’ ä½¿ç”¨ DeepSeek
3. å¦‚æœé…ç½®äº† `QWEN_API_KEY` â†’ ä½¿ç”¨ Qwen
4. å¦‚æœé…ç½®äº† `OPENAI_API_KEY` â†’ ä½¿ç”¨ OpenAI
5. é»˜è®¤ â†’ ä½¿ç”¨ Ollama (éœ€è¦æœ¬åœ°å®‰è£…)

### 3. æ‰‹åŠ¨æŒ‡å®šåç«¯

```bash
# ä½¿ç”¨DeepSeek
LLM_BACKEND=deepseek

# ä½¿ç”¨Qwen
LLM_BACKEND=qwen

# ä½¿ç”¨OpenAI
LLM_BACKEND=openai

# ä½¿ç”¨Ollama
LLM_BACKEND=ollama
```

## ğŸ“¡ APIæ¥å£

### æŸ¥çœ‹å½“å‰åç«¯ä¿¡æ¯

```bash
curl http://localhost:8000/api/v1/system/version
```

å“åº”ï¼š
```json
{
  "name": "Knowledge Base API",
  "version": "1.0.0",
  "components": {
    "llm_backend": "deepseek",
    "llm_model": "deepseek-chat"
  }
}
```

### æŸ¥çœ‹æ‰€æœ‰åç«¯çŠ¶æ€

```bash
curl http://localhost:8000/api/v1/system/llm/backends
```

å“åº”ï¼š
```json
{
  "current_backend": "deepseek",
  "current_model": "deepseek-chat",
  "available_backends": [
    {
      "name": "deepseek",
      "model": "deepseek-chat",
      "healthy": true,
      "api_key_configured": true
    },
    {
      "name": "qwen",
      "model": "qwen-turbo",
      "healthy": false,
      "api_key_configured": false
    },
    {
      "name": "openai",
      "model": "gpt-3.5-turbo",
      "healthy": false,
      "api_key_configured": false
    },
    {
      "name": "ollama",
      "model": "qwen2.5:7b",
      "healthy": false,
      "api_key_configured": false
    }
  ]
}
```

### åŠ¨æ€åˆ‡æ¢åç«¯

```bash
# åˆ‡æ¢åˆ°Qwen
curl -X POST http://localhost:8000/api/v1/system/llm/switch/qwen

# åˆ‡æ¢åˆ°Ollama
curl -X POST http://localhost:8000/api/v1/system/llm/switch/ollama
```

å“åº”ï¼š
```json
{
  "success": true,
  "message": "å·²åˆ‡æ¢åˆ° qwen åç«¯",
  "current_backend": "qwen",
  "current_model": "qwen-turbo"
}
```

## ğŸš€ å„åç«¯é…ç½®æŒ‡å—

### DeepSeek

1. æ³¨å†Œè´¦å·: https://platform.deepseek.com/
2. è·å–API Key
3. é…ç½®ç¯å¢ƒå˜é‡:
```bash
DEEPSEEK_API_KEY=sk-xxxxx
LLM_BACKEND=deepseek
```

**ä¼˜ç‚¹**: æ€§ä»·æ¯”é«˜ï¼Œä¸­æ–‡èƒ½åŠ›å¼º
**ä»·æ ¼**: çº¦ Â¥1/ç™¾ä¸‡tokens

### Qwen (é€šä¹‰åƒé—®)

1. æ³¨å†Œé˜¿é‡Œäº‘è´¦å·: https://dashscope.aliyun.com/
2. å¼€é€šDashScopeæœåŠ¡
3. è·å–API Key
4. é…ç½®ç¯å¢ƒå˜é‡:
```bash
QWEN_API_KEY=sk-xxxxx
LLM_BACKEND=qwen
```

**ä¼˜ç‚¹**: é˜¿é‡Œäº‘ç”Ÿæ€ï¼Œç¨³å®šå¯é 
**ä»·æ ¼**: æŒ‰è°ƒç”¨é‡è®¡è´¹

### OpenAI

1. æ³¨å†ŒOpenAIè´¦å·: https://platform.openai.com/
2. è·å–API Key
3. é…ç½®ç¯å¢ƒå˜é‡:
```bash
OPENAI_API_KEY=sk-xxxxx
OPENAI_MODEL=gpt-3.5-turbo  # æˆ– gpt-4
LLM_BACKEND=openai
```

**ä¼˜ç‚¹**: èƒ½åŠ›æœ€å¼ºï¼Œç”Ÿæ€å®Œå–„
**ä»·æ ¼**: è¾ƒé«˜ï¼ŒæŒ‰tokenè®¡è´¹

### Ollama (æœ¬åœ°æ¨¡å‹)

1. å®‰è£…Ollama: https://ollama.ai/
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

2. ä¸‹è½½æ¨¡å‹:
```bash
ollama pull qwen2.5:7b
# æˆ–å…¶ä»–æ¨¡å‹
ollama pull llama3.1:8b
ollama pull mistral:7b
```

3. å¯åŠ¨OllamaæœåŠ¡:
```bash
ollama serve
```

4. é…ç½®ç¯å¢ƒå˜é‡:
```bash
OLLAMA_MODEL=qwen2.5:7b
LLM_BACKEND=ollama
# æˆ–
USE_LOCAL_LLM=true
```

**ä¼˜ç‚¹**: å®Œå…¨æœ¬åœ°ï¼Œæ— éœ€APIè´¹ç”¨ï¼Œæ•°æ®éšç§
**ç¼ºç‚¹**: éœ€è¦æœ¬åœ°GPUï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢

## ğŸ”„ è‡ªåŠ¨æ•…éšœåˆ‡æ¢

ç³»ç»Ÿæ”¯æŒè‡ªåŠ¨æ•…éšœåˆ‡æ¢ï¼š

1. å½“å‰åç«¯è°ƒç”¨å¤±è´¥æ—¶
2. è‡ªåŠ¨æ£€æµ‹å…¶ä»–å¯ç”¨åç«¯
3. åˆ‡æ¢åˆ°å¤‡ç”¨åç«¯ç»§ç»­æœåŠ¡

ç¤ºä¾‹ï¼šDeepSeek APIå¤±è´¥ â†’ è‡ªåŠ¨åˆ‡æ¢åˆ°Ollamaæœ¬åœ°æ¨¡å‹

## ğŸ’¡ æœ€ä½³å®è·µ

### å¼€å‘ç¯å¢ƒ
```bash
USE_LOCAL_LLM=true
OLLAMA_MODEL=qwen2.5:7b
```
ä½¿ç”¨æœ¬åœ°Ollamaï¼ŒèŠ‚çœAPIè´¹ç”¨

### ç”Ÿäº§ç¯å¢ƒ
```bash
LLM_BACKEND=auto
DEEPSEEK_API_KEY=sk-xxxxx
QWEN_API_KEY=sk-xxxxx
```
é…ç½®å¤šä¸ªåç«¯ï¼Œè‡ªåŠ¨æ•…éšœåˆ‡æ¢

### é«˜æ€§èƒ½éœ€æ±‚
```bash
LLM_BACKEND=openai
OPENAI_MODEL=gpt-4
```
ä½¿ç”¨æœ€å¼ºæ¨¡å‹

### æˆæœ¬ä¼˜åŒ–
```bash
LLM_BACKEND=deepseek
DEEPSEEK_MODEL=deepseek-chat
```
ä½¿ç”¨é«˜æ€§ä»·æ¯”æ¨¡å‹

## ğŸ§ª æµ‹è¯•åç«¯

```bash
# æµ‹è¯•é—®ç­”
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "question": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    "top_k": 3
  }'
```

å“åº”ä¼šåŒ…å«ä½¿ç”¨çš„åç«¯ä¿¡æ¯ï¼š
```json
{
  "answer": "...",
  "backend": "deepseek",
  "model": "deepseek-chat"
}
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| åç«¯ | é€Ÿåº¦ | æˆæœ¬ | ä¸­æ–‡èƒ½åŠ› | éƒ¨ç½²éš¾åº¦ |
|------|------|------|----------|----------|
| DeepSeek | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| Qwen | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| OpenAI | â­â­â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| Ollama | â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­ |

## ğŸ” æ•…éšœæ’æŸ¥

### åç«¯ä¸å¯ç”¨

```bash
# æ£€æŸ¥åç«¯çŠ¶æ€
curl http://localhost:8000/api/v1/system/llm/backends

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/api.log
```

### API Keyé”™è¯¯

æ£€æŸ¥ `.env` æ–‡ä»¶ä¸­çš„API Keyæ˜¯å¦æ­£ç¡®

### Ollamaè¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags

# å¯åŠ¨Ollama
ollama serve
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [DeepSeekæ–‡æ¡£](https://platform.deepseek.com/docs)
- [Qwenæ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)
- [OpenAIæ–‡æ¡£](https://platform.openai.com/docs)
- [Ollamaæ–‡æ¡£](https://github.com/ollama/ollama)

