#!/usr/bin/env python3
"""
çŸ¥è¯†åº“ç®¡ç†å·¥å…·
æä¾›çŸ¥è¯†åº“çš„æŸ¥çœ‹ã€æ¸…ç†ã€é‡å»ºç­‰åŠŸèƒ½
"""

import os
import sys
import json
import argparse
import warnings
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'
os.environ['DISABLE_TELEMETRY'] = 'True'

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from config import config
import chromadb

def show_stats():
    """æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    print("ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 50)
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“
    try:
        client = chromadb.PersistentClient(path=config.VECTOR_STORE_DIR)
        collection = client.get_collection(config.COLLECTION_NAME)
        count = collection.count()
        
        print(f"âœ… å‘é‡æ•°æ®åº“: å·²è¿æ¥")
        print(f"   é›†åˆåç§°: {config.COLLECTION_NAME}")
        print(f"   æ–‡æ¡£å—æ•°é‡: {count}")
        
    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“: æœªåˆå§‹åŒ–æˆ–é”™è¯¯")
        print(f"   é”™è¯¯: {e}")
    
    # æ£€æŸ¥å¤„ç†åçš„æ–‡ä»¶
    stats_file = Path(config.PROCESSED_DIR) / "stats.json"
    if stats_file.exists():
        with open(stats_file, 'r', encoding='utf-8') as f:
            stats = json.load(f)
        
        print(f"\nğŸ“„ å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡æ¡£æ•°: {stats.get('total_documents', 0)}")
        print(f"   æ€»æ–‡æœ¬å—: {stats.get('total_chunks', 0)}")
        print(f"   å¹³å‡å—å¤§å°: {stats.get('avg_chunk_size', 0):.0f} å­—ç¬¦")
        print(f"   æ„å»ºæ—¶é—´: {stats.get('built_at', 'N/A')}")
        print(f"   åµŒå…¥æ¨¡å‹: {stats.get('embedding_model', 'N/A')}")
    
    # æ£€æŸ¥åŸå§‹æ–‡æ¡£
    raw_docs_dir = Path(config.RAW_DOCS_DIR)
    if raw_docs_dir.exists():
        doc_files = list(raw_docs_dir.rglob("*"))
        doc_files = [f for f in doc_files if f.is_file()]
        
        print(f"\nğŸ“ åŸå§‹æ–‡æ¡£:")
        print(f"   æ–‡ä»¶æ•°é‡: {len(doc_files)}")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        extensions = {}
        for f in doc_files:
            ext = f.suffix.lower()
            extensions[ext] = extensions.get(ext, 0) + 1
        
        for ext, count in sorted(extensions.items()):
            print(f"   {ext or '(æ— æ‰©å±•å)'}: {count} ä¸ª")

def list_documents():
    """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
    print("ğŸ“š æ–‡æ¡£åˆ—è¡¨")
    print("=" * 50)
    
    chunks_info_file = Path(config.PROCESSED_DIR) / "chunks_info.json"
    if not chunks_info_file.exists():
        print("âŒ æœªæ‰¾åˆ°æ–‡æ¡£ä¿¡æ¯æ–‡ä»¶")
        return
    
    with open(chunks_info_file, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    # æŒ‰æ¥æºåˆ†ç»„
    sources = {}
    for chunk in chunks:
        source = chunk['metadata'].get('source', 'unknown')
        if source not in sources:
            sources[source] = []
        sources[source].append(chunk)
    
    print(f"å…± {len(sources)} ä¸ªæ–‡æ¡£:\n")
    
    for i, (source, doc_chunks) in enumerate(sorted(sources.items()), 1):
        filename = Path(source).name
        print(f"{i}. {filename}")
        print(f"   è·¯å¾„: {source}")
        print(f"   æ–‡æœ¬å—æ•°: {len(doc_chunks)}")
        print(f"   ç±»å‹: {doc_chunks[0]['metadata'].get('file_type', 'unknown')}")
        print()

def clear_knowledge_base():
    """æ¸…ç©ºçŸ¥è¯†åº“"""
    print("ğŸ—‘ï¸  æ¸…ç©ºçŸ¥è¯†åº“")
    print("=" * 50)
    
    confirm = input("âš ï¸  ç¡®å®šè¦æ¸…ç©ºçŸ¥è¯†åº“å—ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤ï¼(yes/no): ")
    if confirm.lower() != 'yes':
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return
    
    try:
        # åˆ é™¤å‘é‡æ•°æ®åº“
        client = chromadb.PersistentClient(path=config.VECTOR_STORE_DIR)
        try:
            client.delete_collection(config.COLLECTION_NAME)
            print("âœ… å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
        except:
            print("âš ï¸  å‘é‡æ•°æ®åº“é›†åˆä¸å­˜åœ¨")
        
        # æ¸…ç©ºå¤„ç†åçš„æ–‡ä»¶
        processed_dir = Path(config.PROCESSED_DIR)
        if processed_dir.exists():
            for file in processed_dir.glob("*"):
                if file.is_file():
                    file.unlink()
            print("âœ… å¤„ç†æ–‡ä»¶å·²æ¸…ç©º")
        
        print("\nâœ… çŸ¥è¯†åº“å·²æ¸…ç©ºï¼Œè¯·é‡æ–°è¿è¡Œ build_knowledge_base.py æ„å»º")
        
    except Exception as e:
        print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")

def search_test(query: str, top_k: int = 5):
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print(f"ğŸ” æœç´¢æµ‹è¯•: {query}")
    print("=" * 50)
    
    try:
        import torch
        import numpy as np
        from transformers import AutoTokenizer, AutoModel
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        use_local = os.path.exists(config.EMBEDDING_MODEL_PATH)
        
        # åŠ è½½æ¨¡å‹
        print("åŠ è½½åµŒå…¥æ¨¡å‹...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        if use_local:
            print(f"ä»æœ¬åœ°åŠ è½½: {config.EMBEDDING_MODEL_PATH}")
            
            # Suppress the incorrect Mistral regex warning
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore', message='.*fix_mistral_regex.*')
                
                tokenizer = AutoTokenizer.from_pretrained(
                    config.EMBEDDING_MODEL_PATH,
                    local_files_only=True
                )
            
            model = AutoModel.from_pretrained(
                config.EMBEDDING_MODEL_PATH,
                local_files_only=True
            ).to(device)
        else:
            print(f"ä»åœ¨çº¿åŠ è½½: {config.EMBEDDING_MODEL}")
            
            tokenizer = AutoTokenizer.from_pretrained(
                config.EMBEDDING_MODEL
            )
            
            model = AutoModel.from_pretrained(
                config.EMBEDDING_MODEL
            ).to(device)
        
        model.eval()
        print("æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # è¿æ¥æ•°æ®åº“
        print("è¿æ¥å‘é‡æ•°æ®åº“...")
        client = chromadb.PersistentClient(path=config.VECTOR_STORE_DIR)
        collection = client.get_collection(config.COLLECTION_NAME)
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        print("ç”ŸæˆæŸ¥è¯¢å‘é‡...")
        with torch.no_grad():
            encoded = tokenizer(
                query,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(device)
            
            outputs = model(**encoded)
            
            # Use CLS token embedding
            embedding = outputs.last_hidden_state[:, 0]
            
            # Normalize
            embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
            
            query_embedding = embedding.cpu().numpy()[0].tolist()
        
        # æœç´¢
        print("æ‰§è¡Œæœç´¢...")
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )
        
        print(f"\næ‰¾åˆ° {len(results['documents'][0])} ä¸ªç»“æœ:\n")
        
        for i, (doc, metadata, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        ), 1):
            score = 1 - distance
            print(f"{i}. ç›¸ä¼¼åº¦: {score:.4f}")
            print(f"   æ¥æº: {Path(metadata.get('source', '')).name}")
            print(f"   å†…å®¹: {doc[:100]}...")
            print()
        
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    parser = argparse.ArgumentParser(description="çŸ¥è¯†åº“ç®¡ç†å·¥å…·")
    parser.add_argument('command', choices=['stats', 'list', 'clear', 'search'],
                       help='å‘½ä»¤: stats(ç»Ÿè®¡) | list(åˆ—è¡¨) | clear(æ¸…ç©º) | search(æœç´¢)')
    parser.add_argument('--query', '-q', help='æœç´¢æŸ¥è¯¢ï¼ˆç”¨äºsearchå‘½ä»¤ï¼‰')
    parser.add_argument('--top-k', '-k', type=int, default=5, help='è¿”å›ç»“æœæ•°é‡')
    
    args = parser.parse_args()
    
    if args.command == 'stats':
        show_stats()
    elif args.command == 'list':
        list_documents()
    elif args.command == 'clear':
        clear_knowledge_base()
    elif args.command == 'search':
        if not args.query:
            print("âŒ æœç´¢å‘½ä»¤éœ€è¦ --query å‚æ•°")
            sys.exit(1)
        search_test(args.query, args.top_k)

if __name__ == "__main__":
    main()